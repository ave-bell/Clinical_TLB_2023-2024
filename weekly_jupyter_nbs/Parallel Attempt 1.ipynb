{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # standard\n",
    "import numpy as np # standard\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score # for accuracy calculation\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import thermogram_utilities\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CancerType\n",
       "Adenocarcinoma    72\n",
       "Control           51\n",
       "Squamous          46\n",
       "SCLC              16\n",
       "NOS                8\n",
       "Large cell         6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"/Users/avery/OneDrive/Documents/GitHub/Clinical_TLB_2023-2024/lung_cancer_tlb.xlsx\")\n",
    "df['CancerType'] = np.where(df['CancerType'].isna(), 'Control', df['CancerType'])\n",
    "df['CancerType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/avery/OneDrive/Documents/GitHub/Clinical_TLB_2023-2024/lung_cancer_tlb.xlsx\")\n",
    "\n",
    "# replace NA with control\n",
    "df['CancerType'] = np.where(df['CancerType'].isna(), 'Control', df['CancerType'])\n",
    "\n",
    "# get location of cut off values\n",
    "lower_column_index = df.columns.get_loc(\"T51\")\n",
    "upper_column_index = df.columns.get_loc(\"T83.1\")\n",
    "label_column_index = df.columns.get_loc(\"CancerType\")\n",
    "\n",
    "column_indices = np.arange(lower_column_index, upper_column_index)\n",
    "column_indices = np.append(column_indices, 0)\n",
    "column_indices = np.append(column_indices, 1)\n",
    "\n",
    "\n",
    "\n",
    "column_indices = np.append(column_indices, label_column_index)\n",
    "\n",
    "df = df.iloc[:, column_indices]\n",
    "\n",
    "# keep only Control and Adenocarcinoma for analysis\n",
    "df_tree = df[(df['CancerType'] == 'Control') | (df['CancerType'] == 'Adenocarcinoma')]\n",
    "df_tree = df_tree.reset_index(drop=True)\n",
    "\n",
    "df_tree1 = df_tree.rename(columns={'CancerType': 'Response'})\n",
    "\n",
    "df_tree1 = df_tree1.drop([\"sample_id\", 'pub_id'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_cv_sets(df):\n",
    "    \n",
    "    num_rows = df.shape[0]\n",
    "    all_vals = np.arange(0, num_rows)\n",
    "    train_indices = np.random.choice(num_rows, num_rows, replace = True)\n",
    "    test_indices = np.setdiff1d(all_vals, train_indices)\n",
    "    train_set = df.iloc[train_indices, : ]\n",
    "    test_set = df.iloc[test_indices, : ]\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "def train_and_test(hyperparameters, train_set, test_set):\n",
    "    rf = RandomForestClassifier(**hyperparameters)    # Initialize forest with specified parameters\n",
    "\n",
    "    train_set_labels = train_set['Response']\n",
    "    train_set = train_set.drop(\"Response\", axis=1)\n",
    "\n",
    "    rf = rf.fit(train_set, train_set_labels)     # Train the classifier\n",
    "    test_preds = rf.predict(test_set.drop(\"Response\", axis = 1))                 # Make predictions on the test set\n",
    "    balanced_acc = balanced_accuracy_score(test_set['Response'], test_preds)  # Calculate balanced accuracy\n",
    "    return balanced_acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of RandomForestClassifier(max_depth=61, max_features='log2', n_estimators=500)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this, model = train_and_test(hyperparameter_combinations[17], train_set, test_set)\n",
    "\n",
    "model.get_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500, 'max_depth': 61, 'max_features': 'log2'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_combinations[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df_tree1.shape[0]\n",
    "\n",
    "train_set , test_set = bootstrap_cv_sets(df_tree1)\n",
    "\n",
    "hyperparameter_combinations = [\n",
    "    {'n_estimators': 100, 'max_depth': None, 'max_features': None},\n",
    "    {'n_estimators': 100, 'max_depth': N // 2, 'max_features': None},\n",
    "    {'n_estimators': 100, 'max_depth': None, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 100, 'max_depth': N // 2, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 100, 'max_depth': None, 'max_features': 'log2'},\n",
    "    {'n_estimators': 100, 'max_depth': N // 2, 'max_features': 'log2'},\n",
    "    {'n_estimators': 250, 'max_depth': None, 'max_features': None},\n",
    "    {'n_estimators': 250, 'max_depth': N // 2, 'max_features': None},\n",
    "    {'n_estimators': 250, 'max_depth': None, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 250, 'max_depth': N // 2, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 250, 'max_depth': None, 'max_features': 'log2'},\n",
    "    {'n_estimators': 250, 'max_depth': N // 2, 'max_features': 'log2'},\n",
    "    {'n_estimators': 500, 'max_depth': None, 'max_features': None},\n",
    "    {'n_estimators': 500, 'max_depth': N // 2, 'max_features': None},\n",
    "    {'n_estimators': 500, 'max_depth': None, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 500, 'max_depth': N // 2, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 500, 'max_depth': None, 'max_features': 'log2'},\n",
    "    {'n_estimators': 500, 'max_depth': N // 2, 'max_features': 'log2'},\n",
    "    {'n_estimators': 1000, 'max_depth': None, 'max_features': None},\n",
    "    {'n_estimators': 1000, 'max_depth': N // 2, 'max_features': None},\n",
    "    {'n_estimators': 1000, 'max_depth': None, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 1000, 'max_depth': N // 2, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 1000, 'max_depth': None, 'max_features': 'log2'},\n",
    "    {'n_estimators': 1000, 'max_depth': N // 2, 'max_features': 'log2'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:    0.5s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:    1.4s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:    2.4s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:    3.6s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:    5.4s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:    8.2s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:   13.9s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   24.4s finished\n"
     ]
    }
   ],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_test)(params, train_set, test_set) for params in hyperparameter_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6041666666666667"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = bootstrap_cv_sets(df_tree1)\n",
    "\n",
    "train_and_test(hyperparameter_combinations[0], train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100, 'max_depth': None, 'max_features': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_combinations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''results = []\n",
    "for i in range(20):\n",
    "    train_set , test_set = bootstrap_cv_sets(df_tree1)\n",
    "    hyperparam_result = Parallel(n_jobs=-1)(delayed(train_and_test)(params, train_set, test_set) for params in hyperparameter_combinations)\n",
    "    results.append(hyperparam_result)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this, model = train_and_test(hyperparameter_combinations[17], train_set, test_set)\n",
    "\n",
    "model.get_params\n",
    "\n",
    "hyperparameter_combinations[17]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = bootstrap_cv_sets(df_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\avery\\OneDrive\\Documents\\GitHub\\Clinical_TLB_2023-2024\\weekly_jupyter_nbs\\Parallel Attempt 1.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/avery/OneDrive/Documents/GitHub/Clinical_TLB_2023-2024/weekly_jupyter_nbs/Parallel%20Attempt%201.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m clf\u001b[39m.\u001b[39mfit(train\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mCancerType\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpub_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msample_id\u001b[39m\u001b[39m'\u001b[39m], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m), train[\u001b[39m\"\u001b[39m\u001b[39mCancerType\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/avery/OneDrive/Documents/GitHub/Clinical_TLB_2023-2024/weekly_jupyter_nbs/Parallel%20Attempt%201.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m preds \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict_proba(test\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mCancerType\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpub_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msample_id\u001b[39m\u001b[39m'\u001b[39m], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/avery/OneDrive/Documents/GitHub/Clinical_TLB_2023-2024/weekly_jupyter_nbs/Parallel%20Attempt%201.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m auc \u001b[39m=\u001b[39m roc_auc_score(test[\u001b[39m\"\u001b[39;49m\u001b[39mCancerType\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mAdenocarcinoma\u001b[39;49m\u001b[39m\"\u001b[39;49m,preds[:,\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avery/OneDrive/Documents/GitHub/Clinical_TLB_2023-2024/weekly_jupyter_nbs/Parallel%20Attempt%201.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(auc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avery/OneDrive/Documents/GitHub/Clinical_TLB_2023-2024/weekly_jupyter_nbs/Parallel%20Attempt%201.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m clf\u001b[39m.\u001b[39mclasses_\n",
      "File \u001b[1;32mc:\\Users\\avery\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\avery\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:627\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    625\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[0;32m    626\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m--> 627\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    628\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[0;32m    629\u001b[0m         y_true,\n\u001b[0;32m    630\u001b[0m         y_score,\n\u001b[0;32m    631\u001b[0m         average,\n\u001b[0;32m    632\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    633\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    635\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    636\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[0;32m    637\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\avery\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32mc:\\Users\\avery\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    387\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39msample_weight)\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(train.drop(['CancerType', 'pub_id', 'sample_id'], axis = 1), train[\"CancerType\"])\n",
    "\n",
    "preds = clf.predict_proba(test.drop(['CancerType', 'pub_id', 'sample_id'], axis = 1))\n",
    "\n",
    "auc = roc_auc_score(test[\"CancerType\"] ,preds[:,1])\n",
    "\n",
    "print(auc)\n",
    "clf.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Adenocarcinoma', 'Control'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7360248447204969\n"
     ]
    }
   ],
   "source": [
    "#df_tree[\"CancerType\"] = np.where(df_tree[\"CancerType\"] == \"Control\", 1, 0)\n",
    "train[\"CancerType\"] = np.where(train[\"CancerType\"] == \"Control\", 1, 0)\n",
    "test[\"CancerType\"] = np.where(test[\"CancerType\"] == \"Control\", 1, 0)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(train.drop(['CancerType', 'pub_id', 'sample_id'], axis = 1), train[\"CancerType\"])\n",
    "\n",
    "preds = clf.predict_proba(test.drop(['CancerType', 'pub_id', 'sample_id'], axis = 1))\n",
    "\n",
    "auc = roc_auc_score(test[\"CancerType\"],preds[:,1])\n",
    "\n",
    "print(auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
